### KI-Chatbots im Test: Roulette-Strategien und Codebeispiele

**Einleitung**  
Um herauszufinden, wie gut verschiedene KI-Chatbots den Menschen verstehen, habe ich einen Test gestartet. Dabei ging es vor allem darum, ob die viel gepriesenen Roulette-Setzsysteme wirklich zum Gewinnen führen oder ob sie nur Geld kosten – ähnlich wie beim echten Roulette.

**Testaufbau und Vorgehensweise**  
Ich habe einen spontanen Prompt erstellt, um zu sehen, wie die Chatbots auf dieselbe Aufgabe reagieren. Der Test zielte darauf ab, den generierten Code zu überprüfen und festzustellen, ob die Systeme Fehler erkennen und korrigieren können. Zusätzlich wollte ich wissen, ob es möglich ist, aus einer Tagessimulation einen realistischen Spielverlauf für ein Jahr zu erstellen – so wie es in echten Spielbanken üblich ist.

**Ergebnisse und Beobachtungen**  
- **Codegenerierung und Fehlerkorrektur:**  
  Einige Chatbots lieferten von Anfang an funktionierenden Code, während andere kleinere Fehler aufwiesen, die sie auf Nachfrage schnell behoben. Besonders positiv fiel mir der Code von *Claude 3.7* auf, der von sich aus eine ansprechende grafische Darstellung zeigte. Allerdings enthielt dieser Code zunächst einen großen logischen Fehler im „andrucci_system“, der nach Rückfrage korrigiert wurde.

- **Plausibilitätsprüfung:**  
  Die Ergebnisse der anderen Systeme habe ich auf ihre Plausibilität hin geprüft – alle zeigten eine ähnliche Tendenz. Ob die Programme zu 100 % korrekt arbeiten, bleibt dem Urteil des Lesers überlassen.

- **Simulationserweiterung:**  
  Auf die Frage, ob man aus einer Tagessimulation einen realistischen Spielverlauf für ein Jahr erstellen kann, lieferten die meisten Systeme akzeptablen Code. Viele Ansätze waren jedoch etwas dürftig oder vom Thema abgewichen. Auch hier überzeugte *Claude* mit einer guten grafischen Darstellung, abgesehen von einem einmaligen Fehler, der schnell behoben wurde.

- **Systembeschränkungen:**  
  In der kostenlosen Version musste ich den Code in eine Datei auslagern, da die Systeme nur einen begrenzten Output zuließen. Außerdem lag der Code von *Qwen* häufig „am Thema vorbei“.

- **Weitere Anfragen:**  
  Zusätzlich fragte ich Deepseek nach einer „perfekten“ Strategie, die in der Datei *Perfekte_Martingale-Strategie.md* dokumentiert ist.

**Fazit**  
Das Ergebnis des Tests zeigt, dass die Qualität der Antworten stark von der Formulierung des Prompts abhängt. Ein klarer und strukturierter Prompt – oder eine schrittweise Annäherung an die Lösung – kann die Ergebnisse deutlich verbessern. Bei komplexeren Programmen stoßen die Systeme zudem an ihre Grenzen. Dennoch bietet der Test interessante Einblicke in die Stärken und Schwächen moderner KI-Chatbots bei der Codegenerierung und Simulation.

--------------------
Und eine etwas hochtrabende Version 



### KI-Chatbots im Praxistest: Roulette-Strategien und Codegenerierung unter der Lupe

**Einleitung**  
Im Zuge der rasanten Entwicklung künstlicher Intelligenz erscheint es spannend, zu untersuchen, wie unterschiedliche AI-Chatbots auf denselben Input reagieren. Mein Ziel war es, herauszufinden, inwieweit diese Systeme in der Lage sind, menschliche Denkprozesse zu verstehen – und ob sie dabei möglicherweise überhöhte Versprechen, wie beispielsweise bei Roulette-Setzsystemen, halten können. Der Impuls: Es sollte geprüft werden, ob die viel angepriesenen Systeme tatsächlich gewinnversprechend sind oder ob sie – ähnlich wie das Glücksspiel selbst – eher als Geldabzocke zu bewerten sind.

**Testaufbau und Vorgehensweise**  
Den Ausgangsprompt schrieb ich spontan, so wie er mir in den Sinn kam. Dabei ging es in erster Linie darum, verschiedene Chatbots dieselbe Aufgabe lösen zu lassen. Im Fokus stand dabei nicht nur die reine Codegenerierung, sondern auch das Erkennen und Korrigieren von Fehlern sowie die Fähigkeit, auf Nachfragen angemessen zu reagieren. Ein weiterer Aspekt war die Simulation: Neben einer Tagessimulation interessierte mich, ob es möglich sei, daraus einen Spielverlauf mit Jahressimulation zu erstellen – ganz analog zu den Abläufen in echten Spielbanken.

**Ergebnisse und Beobachtungen**  
Die Resultate fielen teils ernüchternd aus:  
- **Codegenerierung und Fehlerkorrektur:**  
  Einige Chatbots lieferten auf Anhieb funktionierenden Code, während andere kleinere Fehler enthielten, die auf Nachfrage zügig korrigiert wurden. Besonders positiv fiel mir der Code von *Claude 3.7* auf. Ohne explizite Aufforderung präsentierte er eine ansprechende grafische Aufbereitung. Allerdings enthielt dieser Code zunächst einen gravierenden Logikfehler in der Definition des „andrucci_system“. Erst nach Rückfrage wurde der Fehler behoben und der korrekte Code ausgegeben.  
- **Vergleich und Plausibilitätsprüfung:**  
  Die Richtigkeit der weiteren Ergebnisse konnte ich überwiegend anhand von Plausibilitätstests verifizieren – alle Lösungen zeigten eine ähnliche Tendenz. Ob die einzelnen Programme jedoch zu 100 % korrekt sind, überlasse ich der Beurteilung des Lesers.  
- **Simulationserweiterungen:**  
  Auf die Frage, ob es möglich sei, aus einer Tagessimulation einen realitätsnahen Spielverlauf mit Jahressimulation zu generieren, fiel der erzeugte Code überwiegend passabel aus – wenngleich viele Ansätze etwas dürftig oder thematisch abweichend waren. Auch hier zeichnete sich *Claude* durch eine herausragende grafische Darstellung aus, wenngleich es auch hier zu einem einzelnen Fehler in der grafischen Ausgabe kam, der jedoch umgehend korrigiert wurde.  
- **Systembedingte Einschränkungen:**  
  Ein wesentlicher Punkt ist, dass in der kostenlosen Version die Systeme in eine Datei ausgelagert werden mussten, um den kompletten Code auszugeben – eine Maßnahme, die auf die Begrenzung der Output-Tokens zurückzuführen ist. Zudem fiel auf, dass der Code von *Qwen* thematisch oft „am Thema vorbei“ lag.  
- **Zusätzliche Anfragen:**  
  Ergänzend habe ich Deepseek zu meiner „perfekten“ Strategie befragt, welche in der Datei *Perfekte_Martingale-Strategie.md* dokumentiert ist.

**Fazit**  
Die Resultate dieses Experiments zeigen, dass das Endergebnis stark von der Formulierung des Ausgangsprompts abhängt. Ein klar strukturierter und korrekt formulierter Prompt – oder eine schrittweise Annäherung an die Problemlösung – könnte die Resultate deutlich verbessern. Zudem offenbart sich, dass bei komplexeren Programmen immer wieder Verarbeitungsgrenzen der jeweiligen Systeme erreicht werden. Nichtsdestotrotz liefert der Test interessante Einblicke in die Stärken und Schwächen moderner KI-Chatbots im Bereich der Codegenerierung und Simulation.